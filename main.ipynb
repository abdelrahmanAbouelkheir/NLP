{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6087dc",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e87e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install pyarabic\n",
    "# %pip install sentencepiece\n",
    "# %pip install tokenizers\n",
    "# %pip install protobuf\n",
    "# %pip install anytree\n",
    "# %pip install SentencePiece\n",
    "# %pip install camel_tools\n",
    "# %pip install deep_translator\n",
    "# %pip install langid\n",
    "# %pip install farasapy\n",
    "# %pip install ipywidgets\n",
    "# %pip install spacy\n",
    "# %pip install nltk\n",
    "# %pip install defaultdict\n",
    "# %pip install tqdm\n",
    "# %pip install re\n",
    "# %pip install sklearn\n",
    "# %install pytorch , see docs from link https://pytorch.org/get-started/locally/#windows-verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e391ee",
   "metadata": {},
   "source": [
    "Text Extraction from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3695c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder extracted successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Files: 100%|██████████| 222/222 [00:03<00:00, 57.44file/s, \u001b[92m لماذا_يخسر_أمواله_كل_من_يراهن_ضد_بنك_اليابان_وكيف_أصبح_صانع_الأرامل]                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Extracted Successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "extracted_text = []\n",
    "\n",
    "# Specify the path to your zip file and the directory to extract it to\n",
    "zip_file_path = 'dataset.zip'\n",
    "extract_to = 'dataset'\n",
    "\n",
    "# Step 1: Unzip the folder\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)  # Extract to a folder\n",
    "\n",
    "print(\"Folder extracted successfully.\\n\")\n",
    "\n",
    "# Step 2: Access the text file\n",
    "extracted_folder = os.path.join(extract_to, 'Al_Mokhbir_Al_Eqtisadi/raw_data')\n",
    "\n",
    "# Collect the list of files first to show progress properly\n",
    "text_files = []\n",
    "\n",
    "# Loop through all files and subdirectories in the folder to collect text files\n",
    "for root, dirs, files in os.walk(extracted_folder):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            text_files.append(file_path)\n",
    "\n",
    "with tqdm(text_files, desc=\"Adding Files\", unit=\"file\") as pbar:\n",
    "    for file_path in pbar:\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        # Open and read the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Appending to the extracted_text list\n",
    "        extracted_text.append({'name': file_name.replace(\".txt\", \"\"), 'content': content})\n",
    "\n",
    "        # Update progress bar dynamically with the current file name being processed\n",
    "        pbar.set_postfix_str(f\"\\033[92m {file_name.replace(\".txt\", \"\")}\")\n",
    "\n",
    "print(\"\\nText Extracted Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb1c57",
   "metadata": {},
   "source": [
    "Dependency Function for English Translation to Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fda17d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aboue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "from deep_translator import GoogleTranslator\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the English spaCy model (handles English entities)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Caching translated words to avoid redundant calls\n",
    "translation_cache = defaultdict(str)\n",
    "\n",
    "translator = GoogleTranslator(source='en', target='ar')\n",
    "\n",
    "# Function to clean up extra spaces in Arabic text and handle \"ل\" + \"ال\" cases\n",
    "def clean_arabic_spacing(text):\n",
    "    text = re.sub(r'\\bال\\s+', 'ال', text)  # Remove spaces between \"ال\" and the following word\n",
    "    text = re.sub(r'\\bل\\s+ال', 'لل', text)  # Merge \"ل\" and \"ال\" into \"لل\"\n",
    "    text = re.sub(r'\\bل\\s+', 'ل', text)  # Remove spaces between \"ل\" and the following word\n",
    "    text = re.sub(r'\\bو\\s+', 'و', text)  # Remove spaces between \"و\" and the following word\n",
    "    return text\n",
    "\n",
    "# Function to detect language and translate English words, while excluding named entities\n",
    "def translate_english_to_arabic(text):\n",
    "    # Initialize the translator\n",
    "    \n",
    "    # Use NLTK's word tokenization (handles punctuation better)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Process the text with spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    named_entities = {ent.text for ent in doc.ents}  # Set of named entities\n",
    "    \n",
    "    # List to hold the final translated text\n",
    "    translated_words = []\n",
    "    \n",
    "    # Temporary list to collect English words for batch translation\n",
    "    english_words = []\n",
    "    word_punctuation_map = {}\n",
    "\n",
    "    # Iterate through each word\n",
    "    for word in words:\n",
    "        # Remove punctuation from the word\n",
    "        word_cleaned = word.strip(string.punctuation)\n",
    "        \n",
    "        try:\n",
    "            # Detect the language of the word\n",
    "            lang, _ = langid.classify(word_cleaned)\n",
    "            \n",
    "            # If the word is in English and not a named entity, collect for translation\n",
    "            if lang == 'en' and word_cleaned not in named_entities:\n",
    "                english_words.append(word_cleaned)\n",
    "                # Store the punctuation of the word\n",
    "                word_punctuation_map[word_cleaned] = ''.join([char for char in word if char in string.punctuation])\n",
    "            else:\n",
    "                # If the word is already in Arabic or is a named entity, keep it as is\n",
    "                translated_words.append(word)\n",
    "        \n",
    "        except:\n",
    "            # In case of an error (e.g., detecting numbers), keep the word as is\n",
    "            translated_words.append(word)\n",
    "\n",
    "    # If there are any English words, translate them in batch\n",
    "    if english_words:\n",
    "        # Convert the list of words into a sentence or chunk\n",
    "        sentence_to_translate = ' '.join(english_words)\n",
    "\n",
    "        # Check cache first to avoid redundant translations\n",
    "        if sentence_to_translate not in translation_cache:\n",
    "            # Translate the entire chunk to Arabic\n",
    "            translated_batch = translator.translate(sentence_to_translate)\n",
    "            # Cache the result for future use\n",
    "            translation_cache[sentence_to_translate] = translated_batch\n",
    "        else:\n",
    "            # If cached, retrieve the translated text\n",
    "            translated_batch = translation_cache[sentence_to_translate]\n",
    "\n",
    "        # Split the translated text into words\n",
    "        translated_batch_words = translated_batch.split()\n",
    "\n",
    "        # Check if the translated batch words match the number of original words\n",
    "        if len(translated_batch_words) == len(english_words):\n",
    "            # Map the translated words back to their original positions\n",
    "            for i, word in enumerate(english_words):\n",
    "                translated_word = translated_batch_words[i]\n",
    "                punctuation = word_punctuation_map.get(word, '')\n",
    "                translated_words.append(translated_word + punctuation)\n",
    "        else:\n",
    "            # Handle mismatch by translating each word individually (fallback mechanism)\n",
    "            for word in english_words:\n",
    "                translated_word = translator.translate(word)\n",
    "                punctuation = word_punctuation_map.get(word, '')\n",
    "                translated_words.append(translated_word + punctuation)\n",
    "\n",
    "    # Join the list back into a string\n",
    "    translated_text = ' '.join(translated_words)\n",
    "    \n",
    "    # Clean up Arabic spacing\n",
    "    return clean_arabic_spacing(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a63eba",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10bba4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaned successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def removeSeparateLetters(text):\n",
    "    # Regex pattern to match standalone Arabic or English letters\n",
    "    pattern = r'(?<!\\S)[\\u0600-\\u06FFa-zA-Z](?!\\S)'\n",
    "\n",
    "    # Remove standalone letters\n",
    "    filtered_text = re.sub(pattern, ' ', text)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "def removeSquareBracketsIncludingWords(text):\n",
    "    return  re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "def removeExtraSpaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def removeSentenceSpaces(text):\n",
    "    return text.replace(\"\\n\",\" \")\n",
    "\n",
    "def removeUnwantedUnicode(text):\n",
    "    # Remove Unicode control characters (e.g., U+200F)\n",
    "    text = re.sub(r'[\\u200E\\u200F]', '', text)\n",
    "    \n",
    "    # Remove non-Arabic and non-English printable characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077Fa-zA-Z0-9\\s.,!?]', '', text)\n",
    "    \n",
    "    # Normalize spaces (remove extra spaces)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "    \n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = text\n",
    "    \n",
    "    cleaned_text = removeSquareBracketsIncludingWords(cleaned_text)\n",
    "\n",
    "    cleaned_text = removeExtraSpaces(cleaned_text)\n",
    "\n",
    "    cleaned_text = removeSentenceSpaces(cleaned_text)\n",
    "  \n",
    "    cleaned_text = removeSeparateLetters(cleaned_text)\n",
    "\n",
    "    cleaned_text = removeUnwantedUnicode(cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "print(\"\\nCleaning Data\\n\")\n",
    "\n",
    "cleaned_text = []\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"output/cleaned_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with tqdm(text_files,bar_format=\"{l_bar}{bar}\") as pbar:\n",
    "    for episode in extracted_text:\n",
    "        \n",
    "        cleaned = clean_text(episode['content'])\n",
    "        cleaned_text.append(cleaned)\n",
    "        \n",
    "        # Generate the output filename\n",
    "        filename = episode['name'] + \".txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the cleaned text to the file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned)\n",
    "        \n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "print(\"\\nData Cleaned successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d58a1",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ccb425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing Data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalization Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tashkeel, strip_small\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "\n",
    "def removeAuthorEndSentence(text):\n",
    "    possibleEndSentences = [\n",
    "        'عجبتكم الحلقه وحابين تشوفونا اشتركوا القناه استنونا حلقات جديده باذن الله سلام',\n",
    "        'وحابين تشوفونا تاني اشتركوا القناه استنونا حلقات جديده باذن الله سلام',\n",
    "        'وحابين تشوفونا اشتركوا القناه استنونا حلقات جديده باذن الله سلام',\n",
    "        'اشتركوا في القناه واستنونا حلقات جديده باذن الله سلام',\n",
    "        'اشتركوا القناه واستنونا حلقات جديده باذن الله سلام',\n",
    "        'اشتركوا القناه استنونا حلقات جديده باذن الله سلام',\n",
    "        'اشتركوا في القناة واستنونا في حلقات جديدة باذن الله سلام',\n",
    "        'القناه استنونا حلقات جديده باذن الله السلام',\n",
    "        'الحلقه وحابين تشوفونا تاني اشتركوا',\n",
    "        'واستنونا حلقات جديده باذن الله سلام',\n",
    "        'واستنونا حلقات جديده باذ الله سلام',\n",
    "        'وحابين تشوفونا تاني',\n",
    "        'تشوفونا تاني',\n",
    "    ]\n",
    "\n",
    "    possibleEndSentences2 = [\n",
    "        'هتابع اجاباتكم التعليقات',\n",
    "        'هتابع في اجاباتكم التعليقات'\n",
    "    ] \n",
    "    \n",
    "    # Define the start point as 90% of the text length to search from\n",
    "    start_point = int(len(text) * 0.8)\n",
    "    \n",
    "    # Search the last 10% of the text for any of the possible end sentences\n",
    "    last_text = text[start_point:]\n",
    "    \n",
    "    for sentence in possibleEndSentences:\n",
    "        if sentence in last_text:\n",
    "            # Remove the found sentence from the full text\n",
    "            text = text.replace(sentence, '')\n",
    "            last_text = text[start_point:]\n",
    "            \n",
    "    \n",
    "    for sentence in possibleEndSentences2:\n",
    "        if sentence in last_text:\n",
    "            # Remove the found sentence from the full text\n",
    "            text = text.replace(sentence, '')\n",
    "            last_text = text[start_point:]\n",
    "            \n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def removeAuthorStartSentence(text):\n",
    "    possibleEndSentences = [\n",
    "      'اشرف ابراهيم','دا المخبر الاقتصادي',\n",
    "    ]\n",
    "    \n",
    "    # Define the start point as 90% of the text length to search from\n",
    "    end_point = int(len(text) * 0.3)\n",
    "    \n",
    "    # Search the last 10% of the text for any of the possible end sentences\n",
    "    last_text = text[:end_point]\n",
    "    \n",
    "    for sentence in possibleEndSentences:\n",
    "        if sentence in last_text:\n",
    "            # Remove the found sentence from the full text\n",
    "            text = text.replace(sentence, '')\n",
    "            last_text = text[:end_point]\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def removeAuthorName(text):\n",
    "    text = text.replace('اشرف','')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def normalizeText(text = \"\",r_s = True):\n",
    "    # normalizedText = translate_english_to_arabic(text)\n",
    "    normalizedText = text\n",
    "\n",
    "    normalizedText = strip_tashkeel(normalizedText)\n",
    "    normalizedText = strip_small(normalizedText)\n",
    "    normalizedText = normalize_alef_ar(normalizedText)\n",
    "    normalizedText = normalize_alef_maksura_ar(normalizedText)\n",
    "    normalizedText = normalize_teh_marbuta_ar(normalizedText)\n",
    "\n",
    "    normalizedText = removeAuthorStartSentence(normalizedText)\n",
    "\n",
    "    normalizedText = removeAuthorEndSentence(normalizedText)\n",
    "    \n",
    "    normalizedText = removeAuthorName(normalizedText)\n",
    "\n",
    "    return normalizedText\n",
    "\n",
    "\n",
    "def prepareArabicStopWords():\n",
    "    # Path to stop-words file\n",
    "    file_path = 'arabic_stop_words.txt'  # Replace with the actual path to your file\n",
    "\n",
    "    # Openening the file and read the contents\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        #Reading all lines, strip to remove extra whitespace and create a list\n",
    "        stopwords = [line.strip() for line in file]\n",
    "        \n",
    "    # normalize\n",
    "    normalizedStopWords = [normalizeText(text=word,r_s=False) for word in stopwords]\n",
    "    \n",
    "    return normalizedStopWords\n",
    "\n",
    "arabic_stopwords = prepareArabicStopWords()\n",
    "\n",
    "normalizedData = []\n",
    "\n",
    "print('\\nNormalizing Data')\n",
    "print()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"output/normalized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with tqdm(text_files,bar_format=\"{l_bar}{bar}\") as pbar:\n",
    "    for index, text in enumerate(cleaned_text):\n",
    "\n",
    "        normalizedText = normalizeText(text)\n",
    "\n",
    "        normalizedData.append(normalizedText)\n",
    "        \n",
    "        filename = extracted_text[index]['name'] + \".txt\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the normalized text to the file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(normalizedText)\n",
    "        \n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"\\nNormalization Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea7885",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0779c826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing Data \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyarabic import araby\n",
    "\n",
    "def tokenize(text):\n",
    "    return araby.tokenize(text)\n",
    "\n",
    "tokenizedDataPhaseOne = []\n",
    "\n",
    "print('\\nTokenizing Data \\n')\n",
    "\n",
    "with tqdm(text_files,bar_format=\"{l_bar}{bar}\") as pbar:\n",
    "    for text in normalizedData:\n",
    "        tokenizedDataPhaseOne.append(tokenize(text))\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "print(\"\\nTokenization Completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271b20e",
   "metadata": {},
   "source": [
    "Step Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بعد', 'سبعه', 'اشهر', 'وانا', 'لا', 'اجرؤ', 'علي', 'حذف', 'شيء', 'يخصك', 'ما', 'اليوم', 'ازيل', 'تطبيقات', 'الالعاب', 'التي', 'حملتم', 'علي', 'جهازي', 'اخرج', 'من', 'المجموعات', 'التعليميه', 'الخاصه', 'بكما', 'الغي', 'متابعتي', 'لكل', 'صفحه', 'اراها', 'امامي', 'تعرض', 'ملابس', 'الاطفال', 'والعابهم', 'لم', 'يكن', 'قتلكم', 'كابوسا', 'كما', 'ظننت', 'بل', 'كان', 'افضع', 'حقيقه', 'هي', 'قصه', 'ام', 'من', 'امهات', 'غزه', 'خسرت', 'اطفالها', 'هي', 'قصه', 'من', '15000', 'قصه', 'لاكثر', 'من', '15000', 'طفل', 'قتلهم', 'الاحت', 'خلوا', 'عينكم', 'علي', 'غزه', 'خلوا', 'عينكم', 'علي', 'رفاح']\n",
      "['اشهر', 'اجرؤ', 'حذف', 'شيء', 'يخصك', 'ازيل', 'تطبيقات', 'الالعاب', 'حملتم', 'جهازي', 'اخرج', 'المجموعات', 'التعليميه', 'الخاصه', 'بكما', 'الغي', 'متابعتي', 'لكل', 'صفحه', 'اراها', 'امامي', 'تعرض', 'ملابس', 'الاطفال', 'والعابهم', 'يكن', 'قتلكم', 'كابوسا', 'ظننت', 'افضع', 'حقيقه', 'قصه', 'امهات', 'غزه', 'خسرت', 'اطفالها', 'قصه', '15000', 'قصه', 'لاكثر', '15000', 'طفل', 'قتلهم', 'الاحت', 'خلوا', 'عينكم', 'غزه', 'خلوا', 'عينكم', 'رفاح']\n"
     ]
    }
   ],
   "source": [
    "def removeStopWords(wordList):\n",
    "    filteredWords = [word for word in wordList if word not in arabic_stopwords]\n",
    "    return filteredWords\n",
    "\n",
    "\n",
    "tokenizedDataPhaseOne = [removeStopWords(item) for item in tokenizedDataPhaseOne]\n",
    "\n",
    "print(\"\\Step words removal Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1e735",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3faf24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mohataher/arabic-stop-words/blob/master/list.txt --> stop words\n",
    "# https://hajar-iba.medium.com/camel-tools-a-python-toolkit-for-arabic-nlp-ba9f1d2e8cb7 --> article link\n",
    "\n",
    "# from farasa.stemmer import FarasaStemmer\n",
    "# stemmer =  FarasaStemmer(interactive=True)\n",
    "\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def stemmize(word):\n",
    "    return stemmer.suf32(word)\n",
    "    # Farasa stemmer, after testing not good output as nltk\n",
    "    # return stemmer.stem(word)  \n",
    "\n",
    "tokenizedDataPhaseOne = [[stemmize(token)for token in tokens] for tokens in tokenizedDataPhaseOne]\n",
    "\n",
    "textAfterPhaseOneTokenization = [' '.join(tokens) for tokens in tokenizedDataPhaseOne]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa77e7",
   "metadata": {},
   "source": [
    "Tokenization but using the tokenizer of the used model from transformers package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e83890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Created\n",
      "Data Tokenization Completed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"faisalq/bert-base-arabic-senpiece\")\n",
    "print(\"Tokenizer Created\")\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text=textAfterPhaseOneTokenization, padding=True,max_length=128 ,truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Data Tokenization Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ed7e4",
   "metadata": {},
   "source": [
    "Create Model & Pass Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a900192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = BertModel.from_pretrained(\"faisalq/bert-base-arabic-senpiece\")\n",
    "print(\"Model Created\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427d3a3",
   "metadata": {},
   "source": [
    "Extract text in the form of numerical representations (embeddings) from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4557bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "import numpy as np\n",
    "\n",
    "# Convert the PyTorch tensor embeddings to a NumPy array\n",
    "embeddings_array = np.stack([embedding.numpy() for embedding in sentence_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f979854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Normalize the embeddings\n",
    "scaler = StandardScaler()\n",
    "normalized_embeddings = scaler.fit_transform(embeddings_array)\n",
    "\n",
    "# Define the number of clusters\n",
    "num_clusters = 6  # Adjust this based on evaluation\n",
    "\n",
    "# Apply K-Means clustering to the embeddings\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init=50, random_state=42)\n",
    "\n",
    "# Fit the model to the normalized embeddings\n",
    "kmeans.fit(normalized_embeddings)\n",
    "\n",
    "# Get the cluster labels for each text\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e03b",
   "metadata": {},
   "source": [
    "Show Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8729554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Clusters\n",
      "\n",
      "├── Cluster 2\n",
      "\n",
      "│   ├── أطفال_غزة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أنقذ_هنري_كسينجر_إسرائيل_من_الهلاك\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_استطاع_الأغنياء_اليهود_إسقاط_رئيسة_أغنى_جامعة_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تستخدم_إسرائيل_نظام_الإنجيل_في_الاغتيالات_الجماعية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تستطيع_أمريكا_إجبار_إسرائيل_على_وقف_حربها_ضد_قطاع_غزة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تسرق_إسرائيل_أموال_الفلسطينين_بخطة_محكمة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تنوي_أمريكا_السيطرة_على_أموال_روسيا_ومساعدة_إسرائيل_هل_ينتقم_بوتين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_حاولت_مجموعة_سرية_من_الأغنياء_الضغط_على_نعمت_لصالح_إسرائيل_وتحريض_الشرطة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_دمرت_المقاومة_الدبابة_الإسرائيلية_الخارقة_هل_تعلمت_إسرائيل_من_روسيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_عاقب_رئيس_كولومبيا_إسرائيل_ولماذا_لا_تنسى_كولومبيا_ما_فعله_الإسرائيليون\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_فاجأت_تركيا_إسرائيل_هل_أصبح_الاقتصاد_الإسرائيلي_في_خطر\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_وجهت_الصين_ضربة_قوية_لأهم_ميناء_في_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يحرم_الإسرائيليون_غزة_من_ثرواتها_الموجودة_في_البحر\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يضرب_الحوثيون_في_اليمن_أضعف_نقطة_في_اقتصاد_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_إسرائيل_من_سحب_أغنى_جامعات_أمريكا_لأموالها_هل_ينتصر_الطلاب_عليها\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخفي_أمريكا_السلاح_الذي_ترسله_لـإسرائيل_هل_تخاف_من_المقاومة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تدافع_هولندا_بشراسة_عن_إسرائيل_كيف_أصبحت_أفضل_صديق_لها_بأوروبا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تدعم_أمريكا_الإسرائيليين_بالمال_والسلاح\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تستخدم_إسرائيل_القنابل_الغبية_بكثافة_في_قصف_غزة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تستعين_إسرائيل_بالشركة_التي_تساعد_أمريكا_في_التجسس_على_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تهاجم_أمريكا_وإسرائيل_تيكتوك_الصيني\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_دمرت_إسرائيل_مطار_غزة_الدولي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_دمرت_إسرائيل_ميناء_غزة_البحري\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_فتحت_أمريكا_لإسرائيل_مخبأ_السلاح_الذي_بنته_لأجل_العرب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لا_تبيع_أمريكا_أحدث_أسلحتها_للدول_العربية_وتعطيها_لإسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لا_يرغب_أحد_في_زيارة_إسرائيل_كيف_انهارت_السياحة_الإسرائيلية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لا_يستطيع_الإسرائيليون_الدخول_في_حرب_طويلة_ما_نقطة_ضعفهم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_هاجم_العراق_وإيران_ناقلات_النفط_في_الخليج_بالصواريخ_والألغام\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_هدد_الإسرائيليون_فرنسا_بإفشاء_السر_الذي_سيقاطعها_العرب_بسببه\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_هربت_إسرائيل_من_قطاع_غزة_هل_يتعلم_نتنياهو_الدرس_من_شارون\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يخاف_الرئيس_الأمريكي_من_إسرائيل_وهل_يخسر_بسببها_الانتخابات\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يرسل_الصرب_السلاح_لإسرائيل_ما_السر_الذي_تخفيه_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يعيش_الإسرائيليون_في_خوف_بالملاجئ_كيف_تفشل_القبة_الحديدية_بحمايتهم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ما_سر_تحريض_إسرائيل_لأمريكا_على_تدمير_أهم_مؤسسة_دولية_في_غزة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ماذا_لو_دخلت_إسرائيل_في_حرب_مع_حزب_الله_في_لبنان_هل_ستكون_حربا_مدمرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__من_سيخسر_لو_قامت_الحرب_بين_إيران_وإسرائيل_كيف_سيكون_شكل_الحرب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تتخلى_أمريكا_عن_إسرائيل_وتتركها_وحدها_كيف_هدد_أنصار_إسرائيل_الرئيس_بايدن\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تحاول_الصين_تدمير_الاقتصاد_الإسرائيلي_لماذا_تخاف_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_كان_من_الممكن_تأسيس_إسرائيل_على_أرض_الصين_لماذا_فشلت_الخطة_الصينية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يستطيع_سلاح_النفط_تدمير_الاقتصاد_الإسرائيلي_ماذا_لو_قطع_البترول_عن_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يقود_العرب_والمسلمون_الأمريكيون_بايدن_لخسارة_الانتخابات_أمام_ترمب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_هل_يسحب_أغنى_صندوق_في_العالم_أمواله_من_إسرائيل_لماذا_تخاف_النرويج_من_رد_أمريكا\n",
      "\n",
      "│   └── المخبر_الاقتصاديلماذا_يتحدى_إيلون_ماسك_واحدة_من_أقوى_المنظمات_اليهودية_في_أمريكاوهل_يخسر_تويتر\n",
      "\n",
      "├── Cluster 5\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___كيف_أصبح_أصدقاء_إسرائيل_من_أقوى_رجال_الظل_في_بريطانيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___كيف_تخطط_أمريكا_لمنع_تطور_قطاع_التكنولوجيا_الصيني_ومن_يجرؤ_على_تحدي_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___كيف_سرقت_بريطانيا_كوه_نور_أو_أشهر_ماسة_في_العالم_ووضعتها_في_تاج_الملكة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___كيف_يمكنك_الحصول_على_شيك_بآلاف_الدولارات_سنويا_لو_عشت_في_هذا_المكان\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___لماذا_يجب_أن_نخاف_من_تطبيق_ثريدز_ومن_صاحبه_مارك_زوكربرغ\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أجبرت_أمريكا_الصين_على_بيع_تطبيق_تيكتوك_لها_أو_تدميره_هل_تبيع_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أصبح_المغرب_حارس_بوابة_الأمن_الغذائي_العالمي_بعد_غزو_روسيا_لأوكرانيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أصبحت_نعمت_شفيق_أشهر_امرأة_في_أمريكا_ما_الذي_تخشاه_نعمت\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_انهار_المشروع_الدفاعي_الإسرائيلي_في_لحظات\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_بنى_محمد_الفايد_ثروته_وكيف_أنقذ_الجنيه_الإسترليني\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تخطط_أقوى_امرأة_في_روسيا_لاستعادة_أموال_بلادها_المحاصرة_في_الغرب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تخطط_إسرائيل_للقضاء_على_مظاهرات_الجامعات_في_أمريكا_لماذا_الطلاب_في_خطر\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تخطط_الصين_لضرب_اقتصاد_أمريكا_بسلاح_العناصر_النادرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تسيطر_الصين_على_قطاع_الأدوية_العالمي_من_خلال_المواد_الفعالة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_حاولت_إسرائيل_قتل_محمد_الضيف_باستخدام_واحدة_من_أقوى_القنابل_الأمريكية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_حاولت_فرنسا_اغتيال_اقتصاد_أغنى_دولة_في_العالم_بخام_البوكسيت\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_خططت_أمريكا_سرا_لتعقيم_سكان_13_دولة_نامية_والسيطرة_على_ثرواتهم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_خططت_بريطانيا_لكسر_منظمة_أوبك_وضرب_مصالح_السعودية_عبر_غزو_العراق\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_سرقت_أمريكا_قناة_بنما_من_أصحابها_هل_قتلت_زعيم_بنما\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_صدرت_أمريكا_الأمراض_للصين_ما_سر_الطعام_الأمريكي_المنتشر_في_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_فاجأت_السعودية_العالم_وأصبحت_على_رأس_لعبة_الأثرياء\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_قضت_فرنسا_على_محاولة_دولة_أفريقية_للخروج_من_هيمنتها\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_نهبت_بريطانيا_64_تريليون_دولار_من_الهند\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_نهبت_فرنسا_مليارات_الدولارات_من_دولة_غنية_وحولتها_لدولة_فقيرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_ينهب_أسطول_الصين_المتوحش_الأسماك_من_40_دولة_هل_يجوع_العالم_لتأكل_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_انهارت_المباني_في_زلزال_تركيا_وسوريا_بمنتهى_السهولة_أين_الكود\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تحالفت_روسيا_مع_كوريا_الشمالية_كيف_يهدد_هذا_التحالف_الاقتصاد_العالمي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_أمريكا_وأوروبا_من_فرض_عقوبات_على_أكبر_شركة_نووية_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_أوروبا_من_الاستيلاء_على_أموال_روسيا_كيف_يخطط_بوتين_للانتقام\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_ترسل_أفريقيا_نصف_احتياطياتها_النقدية_لفرنسا_يوميا_وكيف_تسرقها_باريس\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تقترض_فرنسا_بشراهة_وكيف_تحاول_توفير_المال_للجيش_الذي_قد_يواجه_روسيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تهاجم_ألمانيا_مصالح_فرنسا_الاقتصادية_هل_يكره_الألمان_ماكرون\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_حاربت_أمريكا_المقاطعة_العربية_ولماذا_يقاطع_الناس\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_حاصر_اليهود_الأمريكيون_اقتصاد_المكسيك_وكيف_خضعت_المكسيك\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_دعا_تحالف_بريكس_مصر_والسعودية_والإمارات_للانضمام_إليه\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_ستدفع_بريطانيا_مليارات_من_الإسترليني_لطرد_طالبي_اللجوء_القادمين_من_فرنسا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_قد_تقطع_بريطانيا_السلاح_عن_إسرائيل_قريبا_هل_تخلت_عن_نتنياهو\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لا_تنسى_جنوب_أفريقيا_ما_فعلته_إسرائيل_بها_لماذا_جرتها_للمحاكمة_الدولية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يتصارع_أحفاد_ملوك_البنوك_أو_أقوى_عائلة_مصرفية_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يحب_زعيم_الهند_إسرائيل_بشدة_ما_سر_هذه_العلاقة_الوطيدة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يستطيع_الأمريكيون_شتم_رئيسهم_ولا_يجرؤون_على_انتقاد_إسرائيل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يكره_الفرنسيون_الدولار_الأمريكي_هل_حاولت_فرنسا_تدمير_الدولار\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يكره_الفرنسيون_خطة_ماكرون_ويرفضون_العمل_مثل_الألمان_لنهاية_العمر\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ما_علاقة_أمريكا_بالطائرة_التي_ركبها_رئيس_إيران_في_رحلته_الأخيرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ما_قصة_العبقري_الذي_جعل_سنغافورة_دولة_غنية_جدا_وكيف_استثمر_أموال_الشعب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ماذا_فعلت_فرنسا_في_النيجر_وكيف_يسرق_الفرنسيون_اليورانيوم_من_أفريقيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__من_سيخسر_لو_دخلت_الصين_قريبا_في_حرب_مع_أمريكا_لضم_تايوان_بالقوة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تستطيع_أقوى_امرأة_في_أمريكا_هزيمة_أقوى_امرأة_في_روسيا_من_يستسلم_أولا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تستطيع_أمريكا_وإسرائيل_إجبار_الصين_على_بيع_تيكتوك_أو_سر_الصين_العظيم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_ستعاني_إسرائيل_من_أزمة_غذاء_قريبا_وكيف_يحاصر_قطاعها_الزراعي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يتخلى_مشروع_المغرب_الهائل_عن_بريطانيا_لصالح_ألمانيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يستطيع_إيلون_ماسك_تحديد_من_سيفوز_بمنصب_رئيس_أمريكا_في_الانتخابات_القادمة\n",
      "\n",
      "│   └── كيف_ضاع_حلم_الفقراء_الذين_تمنوا_أن_يصبح_بلدهم_غنيا_مثل_السعودية_بعد_تعرضهم_للنصب\n",
      "\n",
      "├── Cluster 4\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___كيف_بنى_المهاجرون_المغاربيون_اقتصاد_فرنسا_الحديث_وعاشوا_في_جحيم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___لماذا_قد_ينقرض_سكان_كوريا_الجنوبية_قريبا_ولماذا_يرفضون_الزواج\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___لماذا_لا_تخفض_الشركات_أجور_موظفيها_أبدا_وتفضل_الاستغناء_عن_بعضهم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__أين_اختفى_زيت_عباد_الشمس_ولماذا_ترتفع_أسعار_زيوت_الطعام\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_انقطعت_الكهرباء_فجأة_عن_50_مليون_شخص_في_أمريكا_لماذا_ناموا_في_الشوارع\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تتسابق_أمريكا_واليابان_على_صناعة_أقوى_بطارية_في_العالم_هل_تفعلها_تويوتا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تسبب_إفلاس_إف_تي_إكس_في_ضياع_أموال_الملايين_هل_حدثت_خيانة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تضيع_ميراث_أبيك_في_30_ثانية_لماذا_نحب_الفلوس_السهلة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تنصب_على_أذكى_الناس_ألف_مرة_بنفس_الطريقة_وتسرق_أموالهم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_جعل_فاروق_القاسم_النرويج_أغنى_دولة_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يتسبب_العناد_في_ضياع_أموالك_في_استثمارات_فاشلة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يتعامل_الفقراء_مع_المال_وهل_الغني_أذكى_من_الفقير\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_انهارت_البورصات_العالمية_فجأة_وبسرعة_كيف_هز_الين_الياباني_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_سترتفع_أسعار_تذاكر_الطيران_قريبا_هل_تنهار_صناعة_الطيران\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لن_يصبح_99_منا_أثرياء_أبدا_ولماذا_نشعر_بالفقر_طوال_الوقت\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يتسابق_الشعب_في_فيتنام_على_شراء_الذهب_بسرعة_مم_يخاف\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يجب_أن_نحذر_من_شركات_حليب_الأطفال_الصناعي_ماذا_يخفون_عن_الأم_والأب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يجب_أن_يوقف_العالم_إسرائيل_بسرعة_كيف_تستخدم_سلاح_التجويع_ضد_غزة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يجلس_عشرات_الملايين_من_الشباب_الصيني_في_بيوتهم_بدون_عمل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تصنع_أمريكا_عملة_واحدة_بتريليون_دولار_وتسدد_بها_ديونها_للسعودية_والصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_كيف_تخسر_المال_وتتورط_في_الديون_بسبب_البخل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_لماذا_لم_ينهر_قطاع_الطيران_الروسي_رغم_العقوبات_هل_تصنع_روسيا_طائراتها_بنفسها\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_لماذا_يحصل_لاعبو_الكرة_والمطربون_على_رواتب_أكبر_من_أصحاب_الدكتوراة_هل_هذا_عدل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_ماذا_خسرت_فنلندا_بانهيار_العملاقة_نوكيا_وكيف_صنعت_الشركة_دولة\n",
      "\n",
      "│   └── فيلم_قصير_القسط_وبطاقة_الائتمان.._لا_تقع_في_هذا_الفخ\n",
      "\n",
      "├── Cluster 0\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___لماذا_ترفض_شركات_النفط_الأمريكية_إنقاذ_أمريكا_من_قبضة_أوبك_وروسيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___ماذا_لو_دخلت_روسيا_وأمريكا_في_حرب_نووية_بسبب_أوكرانيا_كيف_سيتأثر_العرب\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أخذت_روسيا_شركات_أوروبا_رهينة_مقابل_أموال_بنكها_المركزي_المحاصرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أسس_طباخ_بوتين_أكبر_وأقوى_شركة_عسكرية_خاصة_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_اخترقت_روسيا_الحصار_الأمريكي_لحرمانها_من_إيراداتها_النفطية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تهدد_روسيا_هيمنة_الدولار_الأمريكي_على_العالم_وماذا_عن_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_نجحت_روسيا_مرة_أخرى_في_بناء_احتياطيات_سرية_جديدة_بمليارات_الدولارات\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يهدد_اختفاء_مياه_نهر_الراين_اقتصاد_ألمانيا_وما_أهمية_هذا_النهر_بالتحديد\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_أمريكا_من_رد_بوتين_على_الضربات_القاسية_لروسيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تصر_بعض_الدول_على_تأسيس_صناديق_ثروة_سيادية_وكيف_تدار_ثروات_الدول\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تلجأ_أكبر_شركات_الطاقة_في_أمريكا_إلى_الجزائرهل_حانت_لحظة_الغاز_الجزائري\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_قتلت_روسيا_اتفاقية_الحبوب_هل_ترضخ_أمريكا_وأوروبا_لشروط_بوتين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_لا_ينهار_اقتصاد_روسيا_رغم_العقوبات_وما_هي_ضربة_أمريكا_القادمة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يندم_الألمان_على_ما_فعلوه_مع_روسيا_وكيف_يخنق_بوتين_اقتصادهم_القوي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__ماذا_سيفعل_بوتين_في_كنز_الذهب_الذي_استولت_عليه_روسيا_هل_يرده_لأصحابه\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تستطيع_أمريكا_إجبار_روسيا_على_بيع_نفطها_بعشرة_دولارات_فقط_للبرميل\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تنجح_خطة_أمريكا_وأوروبا_لاستنزاف_اقتصاد_روسيا_لآخر_قطرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تنضم_مصر_والسعودية_والجزائر_لتحالف_بريكس_القوي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يدفع_بوتين_تريليون_دولار_لإعادة_إعمار_أوكرانيا_المدمرة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يستطيع_بوتين_تحدي_فرنسا_وحماية_أصغر_زعيم_في_أفريقيا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يقترب_اقتصاد_روسيا_من_الانهيار_بسبب_العقوبات_الغربية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_ينقذ_غاز_الجزائر_الفرنسيين_من_برد_الشتاء_وماذا_لو_تخلى_الألمان_عن_فرنسا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_كيف_أنقذت_أقوى_امرأة_في_روسيا__اقتصاد_بلادها_من_الانهيار\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_لماذا_تهرب_الشركات_من_ألمانيا_وكيف_تقتل_روسيا_معجزة_أوروبا_الصناعية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي_لماذا_ستختفي_كميات_ضخمة_من_القمح_من_السوق_العالمي_قريبا\n",
      "\n",
      "│   └── المخبر_الاقتصادي_هل_تستطيع_أوروبا_العيش_من_دون_نفط_روسيا_وماذا_لو_انتقم_بوتين\n",
      "\n",
      "├── Cluster 1\n",
      "\n",
      "│   ├── المخبر_الاقتصادي___لماذا_قد_ترتفع_الأسعار_بسرعة_قريبا_من_يقف_وراء_اختفاء_حاويات_الشحن_في_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__أين_اختفت_شركة_توشيبا_اليابانية_لماذا_يبيعها_اليابانيون_بسعر_رخيص\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__أين_اختفت_شركة_هواوي_وكيف_خنقت_أمريكا_عملاق_التكنولوجيا_الصيني\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أصبح_مستقبل_صناعة_السيارات_المغربية_في_أيدي_الصينهل_تصنع_المغرب_البطارية\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_أصبحت_البرازيل_أكبر_مصدر_للدجاج_الرخيص_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تحدت_الصين_أمريكا_وسرقت_أسرار_أخطر_ماكينة_في_العالم_من_هولندا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تخطط_أمريكا_لإعادة_الصين_إلى_العصر_الحجري_هل_تصبح_الصين_دولة_متخلفة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_ترفع_الشركات_أسعار_المنتجات_بدون_أن_يشعر_المستهلكين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_تسعى_أمريكا_لبناء_هذا_المصنع_بسرعة_قبل_غزو_الصين_لتايوان\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_صنعت_الصين_تطبيقا_جديدا_شقيقا_لتيكتوك_ونشرته_بين_الشباب_الأمريكي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_عادت_هواوي_الصينية_لتنتقم_من_أمريكا_وتتحداها_لماذا_لا_تموت_هواوي\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__كيف_يخطط_مؤسس_بي_واي_دي_الصينية_للسيطرة_على_صناعة_السيارات_في_العالم\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تبيع_أمريكا_والنرويج_الغاز_بأسعار_مرتفعة_جدا_لفرنسا_وأوروبا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_أمريكا_من_تيكتوك_ولماذا_ترفض_الصين_بيع_التطبيق_بأي_ثمن\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تخاف_أمريكا_من_هاتف_هواوي_الصينية_الجديد_وكيف_صنعته_الصين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_ترتفع_أسعار_الذهب_بسرعة_لماذا_يشتري_الصينيون_الذهب_بجنون\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_تنقل_آبل_مصانع_الآيفون_من_الصين_إلى_الهند_بسرعة\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_دخلت_أمريكا_والصين_في_صراع_على_المهندسين_ومن_يحمي_هؤلاء_المهندسين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_ستقسم_إمبرطورية_علي_بابا_إلى_شركات_صغيرة_هل_يعاقب_رئيس_الصين_صاحبها\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_قد_تنتصر_الصين_وتكسر_الحصار_الأمريكي_على_هواوي_قريبا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يخاف_إيلون_ماسك_من_إفلاس_تويتر_قريبا_ولماذا_يطرد_الموظفين\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_يرفض_الصينيون_إنفاق_أموالهم_رغم_توسلات_الحكومة_لإنقاذ_الاقتصاد\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__لماذا_ينهار_الين_الياباني_وكيف_أصبح_أرخص_عملة_في_العالم_الغني\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_تضرب_أمريكا_مصانع_أهم_شركة_في_العالم_لردع_الصين_عن_غزو_تايوان\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_خسر_إيلون_ماسك_نصف_ثروته_بسبب_لعنة_تويتر_كيف_ضاعت_أموال_تسلا\n",
      "\n",
      "│   ├── المخبر_الاقتصادي__هل_يستطيع_إيلون_ماسك_تحدي_آبل_كيف_يمكن_لآبل_القضاء_على_تويتر_في_لحظة\n",
      "\n",
      "│   └── المخبر_الاقتصادي_كيف_صنعت_الصين_أحدث_طائرة_ركاب_وهل_تقتل_أمريكا_المشروع\n",
      "\n",
      "└── Cluster 3\n",
      "\n",
      "    ├── المخبر_الاقتصادي___لماذا_قد_ينهار_اقتصاد_أمريكا_لو_استغنت_عن_الصين_من_يملك_خيوط_اللعبة\n",
      "\n",
      "    ├── المخبر_الاقتصادي__أين_اختفت_طائرات_الدولارات_التي_أرسلت_من_أمريكا_للعراق_من_يستطيع_حل_اللغز\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_أذلت_أمريكا_الأرجنتين_حين_رفضت_تسديد_ديونها_ولماذا_هددتها_بالأنسولين\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_أصبح_اليورو_في_خطر_بعد_الاستيلاء_على_أموال_بنك_روسيا_المركزي\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_أصبح_رئيس_البنك_الفدرالي_أخطر_رجل_في_العالم_وأقوى_من_رئيس_أمريكا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تتحكم_أمريكا_في_قرارات_صندوق_النقد_الدولي\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تحركت_الجزائر_ضد_إسرائيل_في_أزمة_حظر_النفط\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تحكم_ثلاث_شركات_خاصة_العالم_كيف_يمكنهما_تدمير_أي_دولة\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تخدعك_إعلانات_شهر_رمضان_ومن_يخسر_بسببها\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تسببت_العملة_الملعونة_لونا_في_خسارة_الملايين_لأموالهم\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تفهم_تحركات_أسعار_الذهب_في_السوق\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_تهدد_الديون_ثروات_دول_كثيرة_في_العالم_كيف_أشعلت_أمريكا_الأزمة\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_جعل_صندوق_النقد_الدولي_الصومال_من_أفقر_دول_العالم_كيف_قتل_الطبيب_المريض\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_جعلت_روسيا_اليورو_أرخص_من_الدولار_هل_يأخذ_بوتين_ثأره\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_دفع_رجل_وامرأة_اقتصاد_بريطانيا_نحو_الانهيار_لماذا_يطبعون_الإسترليني\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_ستعيش_دول_الخليج_بعد_انتهاء_ثروات_النفط_والغاز\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_كشفت_حرب_روسيا_الموقف_الحقيقي_للغرب_تجاه_نفط_وغاز_أفريقيا_وشعوبها_الفقيرة\n",
      "\n",
      "    ├── المخبر_الاقتصادي__كيف_يتحكم_نظام_علاء_الدين_في_ثروات_العالم_ما_سر_خطورة_سلاح_بلاك_روك_القوي\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_ألغت_أمريكا_قاعدة_تحويل_الدولار_إلى_ذهب_كيف_خدعت_العالم_لتنقذ_نفسها\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تبيع_الصين_كميات_كبيرة_من_الدولار_في_السوق_هل_ينهار_اليوان\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تخاف_أمريكا_من_السيارة_الكهربائية_الصينية_هل_تقضي_الصين_على_تسلا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تخزن_الصين_كميات_كبيرة_من_الغذاء_هل_تستغل_أمريكا_نقطة_ضعف_الصين\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تريد_بريطانيا_العودة_للاتحاد_الأوروبي\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تسحب_البنوك_المركزية_احتياطيات_الذهب_من_أمريكا_وبريطانيا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تمتلك_الصين_أكبر_احتياطي_من_الدولار_في_العالم\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تندم_بريطانيا_على_خروجها_من_الاتحاد_الأوروبي_وتلوم_بوتين\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_تنهار_البنوك_الأمريكية_بسرعة_هل_تتكرر_أزمة_2008\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_دخلت_السعودية_في_حرب_أسعار_مع_بريطانيا_في_سوق_النفط\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_زادت_ديون_أمريكا_بسرعة_الصاروخ_ووصلت_إلى_33_تريليون_دولار_متى_تسددها\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_ستخسر_الصين_نصف_سكانها_قريبا_كيف_دمرت_مستقبلها_وقدمت_هدية_لأمريكا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_ستصبح_الحكومة_الأمريكية_بلا_أي_دولارات_قريبا_هل_تبيع_احتياطي_الذهب\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_عجزت_أمريكا_عن_سداد_ديونها_ولماذا_غضب_أصحاب_الديون\n",
      "\n",
      "    ├── المخبر_الاقتصادي__لماذا_فشلت_بريطانيا_في_تقليد_الكويت_والنرويج_وأصبحت_من_أفقر_دول_أوروبا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__ما_الذي_تخفيه_عنك_تطبيقات_المراهنات_كيف_تجعلك_تخسر_أموالك_بهذه_اللعبة\n",
      "\n",
      "    ├── المخبر_الاقتصادي__ما_معنى_الركود_الاقتصادي_وكيف_يؤثر_على_أموال_الناس_ووظائفهم\n",
      "\n",
      "    ├── المخبر_الاقتصادي__ماذا_لو_باع_العرب_النفط_بعملة_غير_الدولار_الأمريكي_هل_تنهار_عملة_أمريكا\n",
      "\n",
      "    ├── المخبر_الاقتصادي__من_دمر_السودان_وجوع_أهله_من_يقف_وراء_أكبر_أزمة_جوع_في_العالم\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_تستطيع_أمريكا_حرمان_السعودية_من_الغذاء_لخفض_أسعار_النفط\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_تستمر_الأسعار_بالارتفاع_خلال_2023_ومتى_تنخفض_بالضبط\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_تطبع_الأرجنتين_صورة_ميسي_على_العملة_لتنقذ_اقتصادها_من_الإفلاس\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_تورط_أمريكا_أوروبا_في_حرب_اقتصادية_مع_روسيا_وكيف_يستفز_الأمريكيون_بوتين\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_ستنخفض_الأسعار_في_2024_متى_يمكن_أن_يحدث_ذلك\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_يحمي_الذهب_أموالك_فعلا_من_التضخم_من_يحرك_أسعار_الذهب\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_يستحق_كأس_العالم_في_قطر_المليارات_التي_صرفت_عليه\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_يصبح_الدينار_عملة_مشتركة_لكل_الدول_العربية_على_غرار_اليورو_الأوروبي\n",
      "\n",
      "    ├── المخبر_الاقتصادي__هل_يلجأ_رئيس_أمريكا_إلى_الحل_الأخير_لكي_ينقذ_بلاده_من_كارثة_اقتصادية\n",
      "\n",
      "    ├── المخبر_الاقتصادي_ماذا_وراء_الشركة_التي_صنعت_أشهر_دواء_في_العالم_كيف_أصبحت_الشركة_أغنى_من_الدولة\n",
      "\n",
      "    └── لماذا_يخسر_أمواله_كل_من_يراهن_ضد_بنك_اليابان_وكيف_أصبح_صانع_الأرامل\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from anytree import Node, RenderTree\n",
    "\n",
    "# Assuming `cluster_labels` and `extracted_text` are already available\n",
    "\n",
    "# Create the root node\n",
    "root = Node(\"All Clusters\")\n",
    "\n",
    "# Dictionary to store the cluster nodes\n",
    "cluster_nodes = {}\n",
    "\n",
    "# Loop through cluster labels and texts\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    \n",
    "    # Create a node for the cluster if it doesn't already exist\n",
    "    if label not in cluster_nodes:\n",
    "        cluster_nodes[label] = Node(f\"Cluster {label}\", parent=root)\n",
    "    \n",
    "    # Add a child node for the text file under the appropriate cluster node\n",
    "    Node(extracted_text[i]['name'], parent=cluster_nodes[label])\n",
    "\n",
    "output_dir = \"output/analysis\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "filename = \"unsupervised_text_classification_by_content\" + \".txt\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "# Check if the file exists, and delete it if it does\n",
    "if os.path.exists(filepath):\n",
    "    os.remove(filepath)\n",
    "\n",
    "# Open the file once in append mode\n",
    "with open(filepath, 'a', encoding='utf-8') as f:\n",
    "    # Render the tree and write to the file\n",
    "    for pre, fill, node in RenderTree(root):\n",
    "        clusteredData = f\"{pre}{node.name}\\n\"\n",
    "        f.write(clusteredData)  # Append each line to the file\n",
    "        print(clusteredData)  # Print the line if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdad3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from transformers import AutoTokenizer, BertModel, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Step 1: Load the Pretrained Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"faisalq/bert-base-arabic-senpiece\")\n",
    "\n",
    "# # Add new tokens (e.g., English words) to the tokenizer\n",
    "# new_tokens = [\"hello\", \"world\", \"AI\", \"machine learning\"]\n",
    "# num_new_tokens = tokenizer.add_tokens(new_tokens)\n",
    "# print(f\"Added {num_new_tokens} new tokens. Updated vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# # Step 2: Load the Pretrained Model and Resize Embeddings\n",
    "# model = BertForSequenceClassification.from_pretrained(\"faisalq/bert-base-arabic-senpiece\", num_labels=2)  # for binary classification\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# print(\"Model embeddings resized to match new vocabulary size.\")\n",
    "\n",
    "# # Step 3: Prepare Dataset\n",
    "# # Load a sample dataset (you can replace it with your own dataset)\n",
    "# dataset = load_dataset(\"glue\", \"sst2\")  # SST-2 dataset for binary sentiment analysis\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "# # Tokenize the dataset for training\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"sentence\"], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Step 4: Define Training Arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",               # Output directory\n",
    "#     evaluation_strategy=\"epoch\",          # Evaluate after each epoch\n",
    "#     learning_rate=2e-5,                   # Learning rate\n",
    "#     per_device_train_batch_size=16,       # Batch size for training\n",
    "#     per_device_eval_batch_size=16,        # Batch size for evaluation\n",
    "#     num_train_epochs=3,                   # Number of epochs\n",
    "#     weight_decay=0.01,                    # Weight decay to prevent overfitting\n",
    "#     logging_dir='./logs',                 # Directory for storing logs\n",
    "#     logging_steps=10,                     # Log every 10 steps\n",
    "# )\n",
    "\n",
    "# # Step 5: Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                          # The model to be trained\n",
    "#     args=training_args,                   # Training arguments\n",
    "#     train_dataset=train_dataset,          # Training dataset\n",
    "#     eval_dataset=test_dataset,            # Evaluation dataset\n",
    "#     tokenizer=tokenizer                   # Tokenizer\n",
    "# )\n",
    "\n",
    "# # Step 6: Fine-Tune the Model\n",
    "# print(\"Starting training...\")\n",
    "# trainer.train()\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# # Step 7: Save the Fine-Tuned Model and Tokenizer\n",
    "# model.save_pretrained(\"./fine_tuned_model\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# # Step 8: Use the Fine-Tuned Model for Inference\n",
    "# inputs = tokenizer(\"السلام عليكم hello world\", return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "# print(\"Inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3272067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, BertForPreTraining\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"faisalq/bert-base-arabic-wordpiece\")\n",
    "# print(\"tokenizer ready\")\n",
    "# model = BertForPreTraining.from_pretrained(\"faisalq/bert-base-arabic-wordpiece\",)\n",
    "# print(\"model ready\")\n",
    "\n",
    "# # sp.encode('This is a test')\n",
    "\n",
    "\n",
    "\n",
    "# # TODO:  Replace with dataset it also takes text\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# print(\"input tokenized\")\n",
    "# outputs = model(**inputs)\n",
    "# print(\"output ready\")\n",
    "# prediction_logits = outputs.prediction_logits\n",
    "# seq_relationship_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "464a6b80-5fba-44e6-9c86-144fd8255910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# # if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# print(os.environ.get(\"GROQ_API_KEY\"))\n",
    "# print(\"ENV preparation done\")\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "\n",
    "# print(\"model creation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c08055d8-224f-4ee0-99df-b1ee74f3f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# messages = [\n",
    "#     SystemMessage(\"Translate the following from English into Italian\"),\n",
    "#     HumanMessage(\"hi!\"),\n",
    "# ]\n",
    "\n",
    "# print(\"message prepared\")\n",
    "\n",
    "# model.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
